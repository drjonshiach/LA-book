(basis-section)=

```{index} Basis
```

# Basis

We have already met the concept of a basis in the chapter on vectors where we saw that [the basis vectors for $\mathbb{R}^3$](basis-vectors-section) are the vectors $\vec{i} = (1, 0, 0)$, $\vec{j} = (0, 1, 0)$ and $\vec{k} = (0, 0, 1)$ and any vector in $\mathbb{R}^3$ can be represented as a linear combination of these basis vectors. The concept of a basis is not limited to Euclidean geometry and we can define a basis for any vector space so that an element in the vector space can be expressed as a {prf:ref}`linear combination<linear-combination-of-vectors-definition>` of the basis elements.

```{index} Spanning set
```

## Spanning sets

```{prf:definition} Spanning set
:label: spanning-set-definition

Let $V$ be a vector space over the field $F$ and $S$ is a subset of $V$. Let $W$ be a subset of $V$ that are expressible as a linear combination of vectors in $S$, i.e., for $u \in W$, $v_1, \ldots, v_n \in S$ and $\alpha_1, \ldots, \alpha_n \in F$

$$ u = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n, $$

then $W$ is a subspace of $V$ and $S$ is a **spanning set** for $W$. We write this as $\operatorname{span}(S)$.
```

For example, $\mathbb{C} = \operatorname{span}(\{1, i\})$ over $\mathbb{R}$ since every element of $\mathbb{C}$ can be expressed as a linear combination of 1 and $i$.

````{prf:example}
:label: spanning-set-example

Show that  $\{ \vec{v}_1, \vec{v}_2 \}$ where $\vec{v}_1 = (2, 1)$ and $\vec{v}_2 = (4, 3)$ is a spanning set for $\mathbb{R}^2$.

```{dropdown} Solution

We need to show that 

$$\alpha_1 \begin{pmatrix} 2\\ 1 \end{pmatrix} + \alpha_2 \begin{pmatrix} 4 \\ 3 \end{pmatrix} = \begin{pmatrix} a\\ b \end{pmatrix},$$ 

for any $a, b \in \mathbb{R}$, i.e., so that the following system has a non-trivial solution.

$$ \begin{align*}
    2 \alpha_1 + 4 \alpha_2 &= a, \\
    \alpha_1 + 3 \alpha_2 &= b. 
\end{align*} $$

A system of linear equations has a solution if it is {prf:ref}`non-singular<inverse-matrix-definition>` (the determinant of the coefficient matrix is non-zero), therefore

$$ \det \begin{pmatrix} 2 & 4 \\ 1 & 3 \end{pmatrix} = 2 \neq 0, $$

so $\{\vec{v}_1, \vec{v}_2\}$ is a spanning set for $\mathbb{R}^2$.
```
````

<!-- The vectors $\vec{i}$, $\vec{j}$ and $\vec{j}$ were introduced in [basis vectors](basis-vectors-section). This leads to the definition of a basis of a vector space. -->

## Basis of a vector space

```{prf:definition} Basis of a vector space
:label: basis-definition
    
A **basis** of a vector space $V$ of a field $F$ is a linearly independent subset of $V$ that spans $V$. A subset $W$ is a basis if it satisfies the following:

- {prf:ref}`linear independence property<linear-dependence-definition>`: for every subset $\{v_1, \ldots , v_n\}$ of $W$ the following equation only has the trivial solution $\alpha_i = 0$.

$$ \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = 0; $$

- {prf:ref}`spanning property<spanning-set-definition>`: for all $u \in V$ we can write $u$ as a linear combination of $v \in W$, i.e.,

$$ u = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n. $$
```

```{index} Basis ; orthogonal basis
```

```{prf:definition} Orthogonal basis
:label: orthogonal-basis-definition

An **orthogonal basis** of a vector space is one in which each of the vectors are orthogonal (perpendicular) to one another.
```

```{index} Basis ; orthonormal basis
```

```{prf:definition} Orthonormal basis
:label: orthonormal-basis-definition

An **orthonormal basis** of a vector space is one in which each of the vectors are orthogonal to one another and each vector is a unit vector.
```

```{index} Vector space ; dimension
```

```{prf:definition} Dimension of a vector space
:label: vector-space-dimension-definition

The **dimension** of a vector space $V$ is denoted by $\dim(V)$ and is the number of elements in the basis for the vector space. 
```

<iframe width="560" height="315" src="https://www.youtube.com/embed/HQBVfacuPOU?si=Sy0YBo-PBfDYaAK4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

````{prf:example}
:label: basis-example

Show that $\{ \vec{v}_1, \vec{v}_2, \vec{v}_3\}$ where $\vec{v}_1 = (1, 1, 0)$, $\vec{v}_2} = (1, -1, 1)$ and $\vec{v}_3 = (1, -1, -2)$ is a basis for $\mathbb{R}^3$.

```{dropdown} Solution

We need to show that the three vectors are linearly independent, i.e., show that the only solution to the following system is $\alpha_1 = \alpha_2 = \alpha_3 = 0$

$$ \begin{align*}
    \alpha_1 \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} +
    \alpha_2 \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} +
    \alpha_3 \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix}  = 
    \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}.
\end{align*} $$

Using Gauss-Jordan elimination

$$ \begin{align*}
    & \left( \begin{array}{ccc|c}
        1 & 1 & 1 & 0 \\
        1 & -1 & 1 & 0\\
        1 & -1 & -2 & 0
    \end{array} \right)
    \begin{matrix} \\ R_2 - R_1 \\ R_3 - R_1 \end{matrix} &
    \longrightarrow &
    \left( \begin{array}{ccc|c}
        1 & 1 & 1 & 0 \\
        0 & -2 & 0 & 0\\
        0 & -2 & -3 & 0
    \end{array} \right)
    \begin{matrix} \\ -\frac{1}{2}R_2 \\ \phantom{x} \end{matrix} \\ \\
    \longrightarrow &
    \left( \begin{array}{ccc|c}
        1 & 1 & 1 & 0 \\
        0 & 1 & 0 & 0\\
        0 & -2 & -3 & 0
    \end{array} \right)
    \begin{matrix} R_1 - R_2 \\ \\ R_3 + 2R_2 \end{matrix}  &
    \longrightarrow &
    \left( \begin{array}{ccc|c}
        1 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0\\
        0 & 0 & -3 & 0
    \end{array} \right)
    \begin{matrix} \\ \phantom{x} \\ -\frac{1}{3}R_3 \end{matrix} \\ \\
    \longrightarrow &
    \left( \begin{array}{ccc|c}
        1 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0
    \end{array} \right)
    \begin{matrix} R_1 - R_3 \\ \phantom{x} \\ \phantom{x} \end{matrix} &
    \longrightarrow &
    \left( \begin{array}{ccc|c}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0
    \end{array} \right)
\end{align*} $$

So $\vec{v}_1$, $\vec{v}_2$ and $\vec{v}_3$ are linearly independent. We also need to show that $\{ \vec{v}_1, \vec{v}_2, \vec{v}_3 \}$ spans $\mathbb{R}^3$.

$$ \det \begin{pmatrix} 1 & 1 & 1 \\ 1 & -1 & -1 \\ 0 & 1 & -2 \end{pmatrix} = 3 + 3 = 6 \neq 0, $$

so $\{ \vec{v}_1, \vec{v}_2, \vec{v}_3 \}$ is a spanning set for $\mathbb{R}^3$. Note that in showing that $\vec{v}_1$, $\vec{v}_2$ and $\vec{v}_3$ are linearly independent we have also shown that $\alpha_1 \vec{v}_1 + \alpha_2 \vec{v}_2 + \alpha_3 \vec{v}_3 = (a, b, c)$ has a solution and $\{ \vec{v}_1, \vec{v}_2, \vec{v}_3 \}$ is a spanning set. Futhermore

$$ \begin{align*}
    \vec{v}_1 \cdot \vec{v}_2 &= \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} = 0, \\
    \vec{v}_1 \cdot \vec{v}_3 &= \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ 2 \end{pmatrix} = 0, \\
    \vec{v}_2 \cdot \vec{v}_3 &= \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} \cdot
    \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix} = 0,
\end{align*} $$

so $\{ \vec{v}_1, \vec{v}_2, \vec{v}_3 \}$ is an orthogonal basis. It is not an orthonormal basis since $\|\vec{v}_1\| = \sqrt{2} \neq 1$.
```
````

```{index} Basis ; change of basis
```

```{index} Basis ; standard basis
```

(change-of-basis-section)=
## Change of basis

$\mathbb{R}^n$ has a particularly nice basis that is easy to write down

$$ E = \left\{ 
        \vec{e}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, 
        \vec{e}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \ldots,
        \vec{e}_n = \begin{pmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix}
        \right\}, $$

which is called the **standard basis**. Note that $\vec{e}_i$ is column $i$ of the identity matrix and for $\mathbb{R}^3$ we have $\vec{i} = \vec{e}_1$, $\vec{j} = \vec{e}_2$ and $\vec{k} = \vec{e}_3$.

We can represent a vector $\vec{u} = (u_1, u_2, \ldots, u_n) \in \mathbb{R}^n$ in the standard basis as a vector with respect to another basis $W = \{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\}$ which is denoted using $[\vec{u}]_W$. Using the standard basis we have

$$u_1 \vec{e}_1 + u_2 \vec{e}_2 + \cdots + u_n \vec{e}_n = \vec{u}, $$

and to express $\vec{u}$ with respect to the basis $W$ we need to solve 

$$  w_1 \vec{v}_1 + w_2 \vec{v}_2 + \cdots + w_n \vec{v}_n = \vec{u}, $$

for $w_1, w_2, \ldots, w_n$. This concept is illustrated for $\mathbb{R}^2$ in {numref}`change-of-basis-figure`. 

```{figure} /_images/5_change_of_basis.svg
:name: change-of-basis-figure
:width: 400

Representing the same point with respect to two different basis.
```

The point with co-ordinates $(u_1, u_2)$ with respect to the standard basis $\{ \vec{e}_1, \vec{e}_2\}$ can be expressed with respect to the basis $\{ \vec{w}_1, \vec{w}_2 \}$ by the co-ordinates $(w_1, w_2)$.

<iframe width="560" height="315" src="https://www.youtube.com/embed/HQBVfacuPOU?si=OZW5J0qBTpuMi1U2&amp;start=126" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

````{prf:example}
:label: change-of-basis-example

Represent the vector $\vec{u} = (4, 0, 5)$ with respect to the basis $W = \{ (1, 1, 0), (1, -1, 1), (1, -1, -2)\}$.

```{dropdown} Solution

We need to solve the system

$$ w_1 \begin{pmatrix} 1 \\ 1 \\ 0 \end{pmatrix} + 
    w_2 \begin{pmatrix} 1 \\ -1 \\ 1 \end{pmatrix} +
    w_3 \begin{pmatrix} 1 \\ -1 \\ -2 \end{pmatrix} =
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix}, $$

which can be written as the matrix equation

$$ \begin{pmatrix}
        1 & 1 & 1 \\
        1 & -1 & -1 \\
        0 & 1 & -2
    \end{pmatrix}
    \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix} =
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix}, $$

Calculating the inverse of the coefficient matrix

$$ \begin{align*}
    \begin{pmatrix} 
        1 & 1 & 1 \\
        1 & -1 & -1 \\
        0 & 1 & -2 
    \end{pmatrix}^{-1} &=
    \frac{1}{6}
    \begin{pmatrix}
        3 & 2 & 1 \\
        3 & -2 & -1 \\
        0 & 2 & -2
    \end{pmatrix}^\mathsf{T} \\
    &= \frac{1}{6} 
    \begin{pmatrix}
        3 & 3 & 0 \\
        2 & -2 & 2 \\
        1 & -1 & -2
    \end{pmatrix} \\
    &=
    \begin{pmatrix} 
        1/2 & 1/2 & 0 \\
        1/3 & -1/3 & 1/3 \\
        1/6 & -1/6 & -1/3
    \end{pmatrix},
\end{align*} $$

so

$$ \begin{align*}
    [\vec{u}]_W = 
    \begin{pmatrix} 
        1/2 & 1/2 & 0 \\
        1/3 & -1/3 & 1/3 \\
        1/6 & -1/6 & -1/3
    \end{pmatrix}
    \begin{pmatrix} 4 \\ 0 \\ 5 \end{pmatrix} =
    \begin{pmatrix} 2 \\ 3 \\ -1 \end{pmatrix}.
\end{align*} $$
```

````

Note that in {prf:ref}`change-of-basis-example` we can represent any vector $\vec{u}$ with respect to the basis $W$ by multiplying by the square matrix in the final equation. This matrix is known as the <a href="https://en.wikipedia.org/wiki/5_change_of_basis" target="_blank">**change of basis matrix**</a>.

```{index} Basis ; change of basis matrix
```

```{prf:definition} Change of basis matrix
:label: change-of-basis-matrix-definition

Let $V$ be a vector space over the field $F$ and $u \in V$. If $E$ and $W$ are two basis for $V$ then the change of basis matrix is the matrix $A_{E \to W}$ such that $[u]_{W} = A_{E \to W} [u]_E$.
```

So to express the vector $\vec{u} \in \mathbb{R}^3$ with respect to the basis $W$ we simply multiply $\vec{u}$ by the change of basis matrix. Changing from a non-standard basis is a slightly more complicated procedure and will be covered in the more advanced units on linear algebra.
