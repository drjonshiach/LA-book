(linear-transformations-chapter)=

:::{index} Linear transformations
:::

# Linear Transformations

You will already be familiar with the use of <a href="https://en.wikipedia.org/wiki/Function_(mathematics)" target="_blank">functions</a> in mathematics to study sets and the **mapping** from a set of inputs set to another set of outputs. The notation used to denote functions is of the form $f: X \to Y$ where $f$ is the name of the function, $X$ is the set of inputs known as the **domain** and $Y$ is a member of the set of outputs known as the **codomain**. The mapping which defines the relationship between the domain and codmain is defined using $y = f(x)$ where $x$ is a member of the domain and $y$ is a member of the codomain. 

In linear algebra we study the linear mapping of a set of vectors to another set of vectors so we define $T: V \to W$ where $V$ and $W$ are vector spaces and the mapping from an input vector $\mathbf{u} \in V$ to an output vector $\mathbf{w} \in W$ is given by $\mathbf{w} = T(\mathbf{u})$.

Linear transformations have lots of uses in mathematics and computing. A good example is in the field of computer graphics and computer games where they are fundamental to the manipulation and visualisation of three-dimensional objects.

We begin with the formal definition of a linear transformation.

:::{prf:definition} Linear transformation
:label: linear-transformation-definition

If $V$ and $W$ are two vector spaces over the same field $F$ then by a **linear transformation** (or **linear mapping**) is a mapping $T: V \to W$ that for any two vectors $\mathbf{u}, \mathbf{v} \in V$ and any scalar $\alpha \in F$ the following conditions hold

- addition operation: $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$;
- scalar multiplication:  $T(\alpha \mathbf{u}) = \alpha T(\mathbf{u})$.

The result of applying a linear transformation to an object is known as the **image**.
:::

For example, let $V = \mathbb{R}^2$ and $W = \mathbb{R}^3$ then $T : V \to W$ defined by $T : (x, y)^\mathsf{T} \mapsto (x, y, 0)^\mathsf{T}$ is a linear transformation. Let $\mathbf{u} = (u_1, u_2)^\mathsf{T}, \mathbf{v} = (v_1, v_2)^\mathsf{T} \in \mathbb{R}^2$ and $\alpha \in F$ then

$$ \begin{align*}
    T \left( \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} + \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} \right)
    &= T \begin{pmatrix} u_1 + v_1 \\ u_2 + v_2 \end{pmatrix}
    = \begin{pmatrix} u_1 + v_1 \\ u_2 + v_2 \\ 0 \end{pmatrix} \\
    &= \begin{pmatrix} u_1 \\ v_1 \\ 0 \end{pmatrix} + \begin{pmatrix} u_2 \\ v_2 \\ 0 \end{pmatrix}
    = T \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} + T \begin{pmatrix} v_1 \\ v_2 \end{pmatrix}.
\end{align*} $$

Similarly

$$ T \left( \alpha \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} \right)
    = T \begin{pmatrix} \alpha u_1 \\ \alpha u_2 \end{pmatrix} 
    = \begin{pmatrix} \alpha u_1 \\ \alpha u_2 \\ 0 \end{pmatrix}
    = \alpha \begin{pmatrix} u_1 \\ u_2 \\ 0 \end{pmatrix}
    = \alpha T \begin{pmatrix} u_1 \\ u_2 \end{pmatrix}. $$

To show whether a transformation is a linear transformation can combine both conditions from the {prf:ref}`definition of a linear transformation<linear-transformation-definition>` to show that

$$ T(\mathbf{u} + \alpha \mathbf{v}) = T(\mathbf{u}) + \alpha T(\mathbf{v}). $$(linear-transformation-condition-equation)

::::{prf:example}
:label: linear-transformation-example

Determine which of the following transformations are linear transformations

(i) &emsp; $T: \mathbb{R}^3 \to \mathbb{R}^2$ defined by $T: (x, y, z)^\mathsf{T} \mapsto (x, y)^\mathsf{T}$

:::{dropdown} Solution (click to show)
Let $\mathbf{u} = (u_1, u_2, u_3)^\mathsf{T}, \mathbf{v} = (v_1, v_2, v_3)^\mathsf{T} \in \mathbb{R}^3$ and $\alpha \in F$ then

$$ \begin{align*}
    T(\mathbf{u} + \alpha \mathbf{v}) &= T\left(
        \begin{pmatrix} u_1 \\ u_2 \\ u_3 \end{pmatrix} + \alpha
        \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}
    \right)
    = T \begin{pmatrix} u_1 + \alpha v_1 \\ u_2 + \alpha v_2 \\ u_3 + \alpha v_3 \end{pmatrix}
    = \begin{pmatrix} u_1 + \alpha v_1 \\  u_2 + \alpha v_2 \end{pmatrix}, \\
    T(\mathbf{u}) + \alpha T(\mathbf{v}) &= T
    \begin{pmatrix} u_1 \\ u_2 \\ u_3 \end{pmatrix} + \alpha T
    \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix} =
    \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} + \alpha
    \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} =
    \begin{pmatrix} u_1 + \alpha v_1 \\ u_2 + \alpha v_2 \end{pmatrix}.
\end{align*} $$

Since $T(\mathbf{u} + \alpha \mathbf{v}) = T(\mathbf{u}) + \alpha T(\mathbf{v})$ then $T: (x, y, z)^\mathsf{T} \mapsto (x, y)^\mathsf{T}$ is a linear transformation.
:::

(ii) &emsp; $T: \mathbb{R}^3 \to \mathbb{R}^2$ defined by $T: (x, y, z)^\mathsf{T} \mapsto (x + 3, y)^\mathsf{T}$

:::{dropdown} Solution (click to show)
Let $\mathbf{u} = (u_1, u_2, u_3)^\mathsf{T}, \mathbf{v} = (v_1, v_2, v_3)^\mathsf{T} \in \mathbb{R}^3$ and $\alpha \in F$ then

$$ \begin{align*}
    T(\mathbf{u} + \alpha \mathbf{v}) &= T 
    \begin{pmatrix} u_1 + \alpha v_1 \\ u_2 + \alpha v_2 \\ u_3 + \alpha v_3 \end{pmatrix} 
    = \begin{pmatrix} u_1 + \alpha v_1 + 3 \\ u_2 + \alpha v_2 \end{pmatrix}, \\
    T(\mathbf{u}) + \alpha T(\mathbf{v}) &= T
    \begin{pmatrix} u_1 \\ u_2 \\ u_3 \end{pmatrix} + \alpha T
    \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix} =
    \begin{pmatrix} u_1 + 3 \\ u_2 \end{pmatrix} + \alpha
    \begin{pmatrix} v_1 + 3 \\ v_2 \end{pmatrix}
    = 
    \begin{pmatrix} u_1 + v_1 + 3 + 3\alpha \\ u_2 + \alpha v_2 \end{pmatrix}
\end{align*} $$

Since $T(\mathbf{u} + \alpha \mathbf{v}) \neq T(\mathbf{u}) + \alpha T(\mathbf{v})$ then $T: (x, y, z)^\mathsf{T} \mapsto (x + 3, y)^\mathsf{T}$ is not a linear transformation. 

Note that we could have shown this by a counterexample, e.g., let $\mathbf{u} = ( 1, 0 , 0 )^\mathsf{T}, \mathbf{v} = (2, 0, 0)^\mathsf{T} \in \mathbb{R}^3$ then

$$ \begin{align*}
    T(\mathbf{u} + \mathbf{v}) &= T
    \begin{pmatrix} 3 \\ 0 \\ 0 \end{pmatrix} 
    = \begin{pmatrix} 6 \\ 0 \end{pmatrix}, \\
    T(\mathbf{u}) + T(\mathbf{v}) &=
    \begin{pmatrix} 4 \\ 0 \\ 0 \end{pmatrix} + \begin{pmatrix} 5 \\ 0 \\ 0 \end{pmatrix}
    = \begin{pmatrix} 9 \\ 0 \end{pmatrix}.
\end{align*} $$
:::

(iii) &emsp; $T: P(\mathbb{R}) \to P(\mathbb{R})$ defined by $T: p \mapsto p \dfrac{\mathrm{d}p}{\mathrm{d}x}$

:::{dropdown} Solution (click to show)

Let $u = x \in P(\mathbb{R})$ then

$$ \begin{align*}
    T(2u) &= T(2x) = 2x(2) = 4x, \\
    2T(u) &= 2T(x) = 2(x)(1) = 2x,
\end{align*}  $$

therefore $T(2u) \neq 2T(u)$ and $T: p \mapsto p \dfrac{\mathrm{d}p}{\mathrm{d}x}$ is not a linear transformation.
:::
::::

(transformation-matrix-section)=

:::{index} Linear transformations ; transformation matrices
:::

## Transformation matrices

For convenience we tend to use matrices to represent linear transformations. Let $T: V \to W$ be a linear transformation from the vector spaces $V$ to $W$ where $V, W \in \mathbb{R}^n$. If $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$ is a basis for $V$ then for a vector $\mathbf{u} \in V$

$$ \mathbf{u} = u_1 \mathbf{v}_1 + u_2 \mathbf{v}_2 + \cdots + u_n \mathbf{v}_n, $$

and by the {prf:ref}`definition of a linear transformation<linear-transformation-definition>` we can apply a linear transformation $T$ to the vectors $\mathbf{u}$ and $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$

$$ T(\mathbf{u}) = u_1 T(\mathbf{v}_1) + u_2 T(\mathbf{v}_2) + \cdots + u_n T(\mathbf{v}_n), $$

so $T(\mathbf{u})$ depends on the vectors $T(\mathbf{v}_1), T(\mathbf{v}_2), \ldots, T(\mathbf{v}_n)$. We can write this as the matrix equation

$$ \begin{align*}
    T(\mathbf{u}) &= \begin{pmatrix} 
        \uparrow & \uparrow & & \uparrow \\
        T(\mathbf{v}_1) & T(\mathbf{v}_2) & \cdots & T(\mathbf{v}_n) \\
        \downarrow & \downarrow & & \downarrow
    \end{pmatrix}
    \begin{pmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{pmatrix} 
    = A \mathbf{u}.
\end{align*} $$

In other words we can apply a linear transformation simply by multiplying $\mathbf{u}$ by a matrix $A$.

:::{prf:definition} Transformation matrix
:label: transformation-matrix-definition

Let $T : V \to W$ be a linear transformation and $A$ be a matrix such that

$$ A = \begin{pmatrix} 
    \uparrow & \uparrow & & \uparrow \\
    T(\mathbf{v}_1) & T(\mathbf{v}_2) & \cdots & T(\mathbf{v}_n) \\
    \downarrow & \downarrow & & \downarrow
\end{pmatrix} $$

then

$$ T(\mathbf{u}) = A\mathbf{u}. $$

$A$ is said to be the **matrix representation of the linear transformation** $T$ (also known as the **transformation matrix**).
:::

::::{prf:example}
:label: transformation-matrix-example

A linear transformation $T:\mathbb{R}^2 \to \mathbb{R}^2$ is defined by $T: (x, y)^\mathsf{T} \mapsto (3x + y, x + 2y)^\mathsf{T}$. Calculate the transformation matrix and use it to calculate $T(1,1)^\mathsf{T}$.

:::{dropdown} Solution (click to show)

Since we are mapping from $\mathbb{R}^2$ the transformation matrix is

$$ A = \begin{pmatrix} T(\mathbf{e}_1) & T(\mathbf{e}_2) \end{pmatrix} $$

Applying the transformation to the standard basis vectors

$$ \begin{align*}
    T(\mathbf{e}_1) = T\begin{pmatrix} 1 \\ 0 \end{pmatrix} 
    = \begin{pmatrix} 3(1) + 0 \\ 1 + 2(0) \end{pmatrix} 
    =  \begin{pmatrix} 3 \\ 1 \end{pmatrix}, \\
    T(\mathbf{e}_2) = T\begin{pmatrix} 0 \\ 1 \end{pmatrix} 
    = \begin{pmatrix} 3(0) + 1 \\ 0 + 2(1) \end{pmatrix} 
    = \begin{pmatrix} 1 \\ 2 \end{pmatrix},
\end{align*} $$

so the transformation matrix is

$$ A = \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix}. $$

Applying the transformation matrix to $(1, 1)^\mathsf{T}$

$$ T\begin{pmatrix} 1 \\ 1 \end{pmatrix} = A \cdot \begin{pmatrix} 1 \\ 1 \end{pmatrix} =  
    \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix} 
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 
    \begin{pmatrix} 4 \\ 3 \end{pmatrix}. $$
:::
::::

The affects of the linear transformation from {prf:ref}`linear-transformation-example` is illustrated in {numref}`linear-transformation-figure`. Note that the transformation $T$ can be thought of as changing the basis of the vector space. The unit square with respect to the basis $\{\mathbf{e}_1, \mathbf{e}_1\}$ has been transformed into a unit parallelogram with respect to the basis $\{ T(\mathbf{e}_1), T(\mathbf{e}_2)\}$.

:::{figure} /images/linear_transformation.svg
:name: linear-transformation-figure
:width: 400

The affect of applying a linear transformation $T: (x,y)^\mathsf{T} \mapsto (3x + y, x + 2y)^\mathsf{T}$ to the vector $(1,1)^\mathsf{T}$.
:::

## Finding the transformation matrix from a set of images

The calculation of the transformation matrix in {prf:ref}`transformation-matrix-example` was straightforward as we knew what the transformation was. This will not always be a the case and we may know what the output of the transformation (known as the image) is but not the transformation itself. Consider a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ applied to vectors $\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_n$. If $A$ is the transformation matrix for $T$ then

$$ \begin{align*}
    A
    \begin{pmatrix}
        \uparrow & \uparrow & & \uparrow \\
        \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_n \\
        \downarrow & \downarrow & & \downarrow
    \end{pmatrix} = 
    \begin{pmatrix}
        \uparrow & \uparrow & & \uparrow \\
        T(\mathbf{u}_1) & T(\mathbf{u}_2) & \cdots & T(\mathbf{u}_n) \\
        \downarrow & \downarrow & & \downarrow
    \end{pmatrix}
\end{align*} $$

therefore

$$ \begin{align*}
    A &=  
    \begin{pmatrix}
        \uparrow & \uparrow & & \uparrow \\
        T(\mathbf{u}_1) & T(\mathbf{u}_2) & \cdots & T(\mathbf{u}_n) \\
        \downarrow & \downarrow & & \downarrow
    \end{pmatrix}
    \begin{pmatrix}
        \uparrow & \uparrow & & \uparrow \\
        \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_n \\
        \downarrow & \downarrow & & \downarrow
    \end{pmatrix}^{-1}
\end{align*} $$

:::{prf:theorem} Determining the linear transformation given the inputs and image vectors
:label: finding-transformation-matrix-theorem

Given a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ applied to a set of $n$ vectors $\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_n$ with known image vectors $T(\mathbf{u}_1), T(\mathbf{u}_2), \ldots, T(\mathbf{u}_n)$ then the transformation matrix $A$ for $T$ is

$$ A = \begin{pmatrix} T(\mathbf{u}_1) & T(\mathbf{u}_2) & \cdots & T(\mathbf{u}_n) \end{pmatrix}  
\begin{pmatrix} \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_n \end{pmatrix}^{-1}. $$(determining-the-transformation-matrix)
:::

::::{prf:example}
:label: transformation-matrix-example-2

Determine the transformation matrix $A$ for the linear transformation $T$ such that

$$ \begin{align*}
    T\begin{pmatrix} 1 \\ 1 \end{pmatrix} &= \begin{pmatrix} 4 \\ 3 \end{pmatrix}, &
    T\begin{pmatrix} 1 \\ 2 \end{pmatrix} &= \begin{pmatrix} 5 \\ 5 \end{pmatrix}.
\end{align*} $$

:::{dropdown} Solution (click to show)

The inverse of $(\mathbf{u}_1, \mathbf{u}_2)$ is

$$ \begin{align*}
    \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}^{-1} &= \begin{pmatrix} 2 & -1 \\ -1 & 1 \end{pmatrix} 
\end{align*} $$

 Right multiplying the image matrix

$$ \begin{align*}
    A &= \begin{pmatrix} 4 & 5 \\ 3 & 5 \end{pmatrix} \begin{pmatrix} 2 & -1 \\ -1 & 1 \end{pmatrix}
    = \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix}.
\end{align*} $$

This is the transformation matrix from {prf:ref}`transformation-matrix-example`.
:::
::::

:::{index} Linear transformations ; inverse transformation
:::

## Inverse linear transformation

:::{prf:definition} Inverse linear transformation
:label: inverse-transformation-definition

Let $T: V \to W$ be a linear transformation with the transformation matrix $A$ then $T$ has an inverse transformation denoted by $T^{-1}: W \to V$ which reverses the affects of $T$. If $\mathbf{u} \in V$ and $\mathbf{v} \in W$ then

$$ \begin{align*}
    \mathbf{v} &= A \mathbf{u} \\
    \therefore \mathbf{u} & = A^{-1}\mathbf{v},
\end{align*} $$

where $A^{-1}$ is the transformation matrix for $T^{-1}$.
:::

::::{prf:example}
:label: inverse-transformation-example

Determine the inverse of the transformation $T: \mathbb{R}^2 \to \mathbb{R}^2$ defined by $T(x, y)^\mathsf{T} \mapsto (3 x + y, x + 2 y)^\mathsf{T}$ and calculate $T^{-1}(4,3)^\mathsf{T}$.

:::{dropdown} Solution (click to show)
We saw in {prf:ref}`transformation-matrix-example` that the transformation matrix for $T$ is

$$ A = \begin{pmatrix} 3 & 1 \\ 1 & 2 \end{pmatrix}, $$

which has the inverse

$$ A^{-1} = \frac{1}{5} \begin{pmatrix} 2 & -1 \\ -1 & 3 \end{pmatrix}. $$

Determining the inverse transformation

$$ \begin{align*}
    T^{-1}\begin{pmatrix} x \\ y \end{pmatrix} &= A^{-1} \cdot \begin{pmatrix} x \\ y \end{pmatrix} =
    \frac{1}{5} \begin{pmatrix} 2 & -1 \\ -1 & 3 \end{pmatrix}
    \begin{pmatrix} x \\ y \end{pmatrix} \\
    &= \begin{pmatrix} \frac{2}{5}x - \frac{1}{5}y \\ -\frac{1}{5}x + \frac{3}{5}y \end{pmatrix}.
\end{align*} $$

Calculating $T^{-1}\begin{pmatrix} 4 \\ 3 \end{pmatrix}$

$$ \begin{align*}
    A^{-1} \begin{pmatrix} 4 \\ 3 \end{pmatrix} &=
   \frac{1}{5} \begin{pmatrix} 2 & -1 \\ -1 & 3 \end{pmatrix}
    \begin{pmatrix} 4 \\ 3 \end{pmatrix} 
    = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\end{align*} $$
:::
::::
